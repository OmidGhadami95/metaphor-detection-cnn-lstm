{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmidGhadami95/metaphor-detection-cnn-lstm/blob/main/CNN_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bWig-9yEcUM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import f1_score, classification_report, precision_score, recall_score\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Conv1D, GlobalMaxPooling1D, Concatenate, Add, LayerNormalization, Dropout, Reshape, Embedding, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, Callback\n",
        "from tensorflow.keras import backend as K\n",
        "import pickle\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation)).lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tags]\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed = [stemmer.stem(word) for word in lemmatized]\n",
        "    return ' '.join(stemmed)\n",
        "\n",
        "def create_complex_model(input_shape, num_classes, max_words):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Embedding(max_words, 128)(inputs)\n",
        "    conv1 = Conv1D(128, 5, activation='relu', padding='same')(x)\n",
        "    conv2 = Conv1D(128, 3, activation='relu', padding='same')(x)\n",
        "    conv3 = Conv1D(128, 7, activation='relu', padding='same')(x)\n",
        "    x = Concatenate()([conv1, conv2, conv3])\n",
        "    x = GlobalMaxPooling1D()(x)\n",
        "    for i in range(5):\n",
        "        residual = x\n",
        "        x = Dense(256, activation='relu')(x)\n",
        "        x = LayerNormalization()(x)\n",
        "        x = Dropout(0.3)(x)\n",
        "        x = Dense(256, activation='relu')(x)\n",
        "        x = LayerNormalization()(x)\n",
        "        if i > 0:\n",
        "            x = Add()([x, residual])\n",
        "    x = Reshape((1, -1))(x)\n",
        "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
        "    x = Bidirectional(LSTM(64))(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    outputs = Dense(num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "class AccuracyThresholdCallback(Callback):\n",
        "    def __init__(self, threshold):\n",
        "        super(AccuracyThresholdCallback, self).__init__()\n",
        "        self.threshold = threshold\n",
        "        self.stopped_epoch = 0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs.get('val_accuracy') > self.threshold:\n",
        "            self.stopped_epoch = epoch + 1\n",
        "            self.model.stop_training = True\n",
        "\n",
        "def f1_macro(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def calculate_metrics(model, X_test_pad, y_test, y_test_cat):\n",
        "    y_pred = model.predict(X_test_pad)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    val_accuracy = np.mean(y_pred_classes == y_test)\n",
        "    val_f1_macro = f1_score(y_test, y_pred_classes, average='macro')\n",
        "    val_precision = precision_score(y_test, y_pred_classes, average='macro')\n",
        "    val_recall = recall_score(y_test, y_pred_classes, average='macro')\n",
        "\n",
        "    return val_accuracy, val_f1_macro, val_precision, val_recall\n",
        "\n",
        "def main(input_file):\n",
        "    df = pd.read_csv(input_file)\n",
        "    df = df.drop_duplicates()\n",
        "    df['processed_text'] = df['text'].apply(preprocess_text)\n",
        "    df = df[df['processed_text'].str.strip() != '']\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    if df['label'].dtype == bool:\n",
        "        df['label'] = df['label'].map({True: 'True', False: 'False'})\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    df['encoded_label'] = le.fit_transform(df['label'])\n",
        "\n",
        "    X = df['processed_text']\n",
        "    y = df['encoded_label']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    max_words = 20000\n",
        "    max_len = 300\n",
        "    tokenizer = Tokenizer(num_words=max_words)\n",
        "    tokenizer.fit_on_texts(X_train)\n",
        "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "    X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "    X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
        "    X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
        "\n",
        "    num_classes = len(le.classes_)\n",
        "    y_train_cat = to_categorical(y_train, num_classes)\n",
        "    y_test_cat = to_categorical(y_test, num_classes)\n",
        "\n",
        "    model = create_complex_model((max_len,), num_classes, max_words)\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy', f1_macro])\n",
        "\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=5, min_lr=0.0001)\n",
        "    accuracy_threshold = AccuracyThresholdCallback(threshold=0.83)\n",
        "\n",
        "    history = model.fit(X_train_pad, y_train_cat, epochs=100, batch_size=32, validation_split=0.2,\n",
        "                        callbacks=[reduce_lr, accuracy_threshold], verbose=0)\n",
        "\n",
        "    # Calculate and print final metrics\n",
        "    val_accuracy, val_f1_macro, val_precision, val_recall = calculate_metrics(model, X_test_pad, y_test, y_test_cat)\n",
        "\n",
        "    print(f\"Training stopped at epoch {accuracy_threshold.stopped_epoch}\")\n",
        "    print(f\"Final Validation Accuracy: {val_accuracy:.4f}\")\n",
        "    print(f\"Final Validation F1 Macro: {val_f1_macro:.4f}\")\n",
        "    print(f\"Final Validation Precision: {val_precision:.4f}\")\n",
        "    print(f\"Final Validation Recall: {val_recall:.4f}\")\n",
        "\n",
        "    # Save the model\n",
        "    with open('saved_model.pkl', 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     if len(sys.argv) != 2:\n",
        "#         print(\"Usage: python3 run_train.py <input_file>\")\n",
        "#         sys.exit(1)\n",
        "\n",
        "#     input_file = sys.argv[1]\n",
        "#     main(input_file)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = 'train-1.csv'  # Replace with the path to your dataset\n",
        "    main(input_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpHJGieAOlLU",
        "outputId": "ef674238-04a2-462a-d2e0-e660504aa903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step\n",
            "Training stopped at epoch 45\n",
            "Final Validation Accuracy: 0.7936\n",
            "Final Validation F1 Macro: 0.6923\n",
            "Final Validation Precision: 0.7136\n",
            "Final Validation Recall: 0.6793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rZEJ5lFjlsc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hicbQ1fcr0Jv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyN+k/oizwr7hZKimW/FlSuX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}